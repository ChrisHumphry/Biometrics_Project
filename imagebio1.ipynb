{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cidSeyf9q3uu"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def clahe1(img, block_size=8, clip_limit=2.0):\n",
        "    # Convert the input image to grayscale\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Compute the image size\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    # Compute the size of each small region\n",
        "    sx, sy = int(w/block_size), int(h/block_size)\n",
        "\n",
        "    # Initialize the output image\n",
        "    out = np.zeros_like(img)\n",
        "\n",
        "    # Loop over each small region\n",
        "    for i in range(block_size):\n",
        "        for j in range(block_size):\n",
        "            # Compute the coordinates of the current region\n",
        "            x, y = j*sx, i*sy\n",
        "\n",
        "            # Extract the current region\n",
        "            region = img[y:y+sy, x:x+sx]\n",
        "\n",
        "            # Compute the histogram for the current region\n",
        "            hist, bins = np.histogram(region.flatten(), 256, [0,256])\n",
        "            cdf = hist.cumsum()\n",
        "            cdf_normalized = cdf / cdf[-1]\n",
        "\n",
        "            # Compute the excess and clip the histogram\n",
        "            w = block_size*block_size\n",
        "            excess = 0\n",
        "            for k in range(256):\n",
        "                if cdf_normalized[k] > clip_limit/w:\n",
        "                    excess += cdf_normalized[k] - clip_limit/w\n",
        "                    cdf_normalized[k] = clip_limit/w\n",
        "            for k in range(256):\n",
        "                cdf_normalized[k] += excess/256\n",
        "\n",
        "            # Reallocate pixel values using the cdf\n",
        "            region_clahe = np.interp(region.flatten(), bins[:-1], cdf_normalized*255).reshape(region.shape)\n",
        "\n",
        "            # Insert the enhanced region into the output image\n",
        "            out[y:y+sy, x:x+sx] = region_clahe\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uWz8h_nXq9X5"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.type_check import imag\n",
        "\n",
        "def clahe2(img):\n",
        "  image = cv2.resize(img, (500, 600))\n",
        "  \n",
        "  # The initial processing of the image\n",
        "  # image = cv2.medianBlur(image, 3)\n",
        "  image_bw = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "  # The declaration of CLAHE\n",
        "  # clipLimit -> Threshold for contrast limiting\n",
        "  clahe = cv2.createCLAHE(clipLimit = 5)\n",
        "  final_img = clahe.apply(image_bw) + 30\n",
        "  \n",
        "  # Ordinary thresholding the same image\n",
        "  _, ordinary_img = cv2.threshold(image_bw, 155, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "  med_final = cv2.medianBlur(final_img, 3)\n",
        "\n",
        "\n",
        "  return med_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WIWSLioCrN0A"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def pca(img):\n",
        "# Load finger vein image\n",
        "  #img = cv2.imread('download.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "  # Convert image to vector form\n",
        "  img_1d = img.reshape(-1)\n",
        "\n",
        "  # Apply PCA\n",
        "  pca = PCA(n_components=0.5)\n",
        "  pca.fit(img_1d.reshape(-1, 1))\n",
        "  img_pca = pca.transform(img_1d.reshape(-1, 1)).reshape(img.shape[0], img.shape[1], -1)\n",
        "\n",
        "  return img_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_dir = ['1st_session/raw_data', '2nd_session/raw_data']\n",
        "preprocessed_dir_prefix = 'preprocessed/'\n",
        "clahe_folder = 'clahe'\n",
        "pca_folder = 'pca'\n",
        "if not(os.path.exists(preprocessed_dir_prefix)): os.makedirs(preprocessed_dir_prefix)\n",
        "\n",
        "for folder in img_dir:\n",
        "    for folder_name in os.listdir(folder):\n",
        "        folder_path = os.path.join(folder, folder_name)\n",
        "        if '.DS_Store' in folder_name:\n",
        "            continue\n",
        "        for img_name in os.listdir(folder_path):\n",
        "            if '.jpg' in img_name:\n",
        "                roi = cv2.imread(os.path.join(folder_path, img_name))\n",
        "                img1 = clahe2(roi)\n",
        "                f_img1 = pca(img1)\n",
        "                path_clahe = os.path.join(preprocessed_dir_prefix, clahe_folder, folder_path)\n",
        "                path_pca = os.path.join(preprocessed_dir_prefix, pca_folder, folder_path)\n",
        "                if not(os.path.exists(path_clahe)): os.makedirs(path_clahe)\n",
        "                if not(os.path.exists(path_pca)): os.makedirs(path_pca)\n",
        "                cv2.imwrite(os.path.join(path_clahe, img_name), img1)\n",
        "                cv2.imwrite(os.path.join(path_pca, img_name), f_img1)\n",
        "            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(25088, 512),\n",
        "            nn.ReLU())\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TripletImageDataset(datasets.ImageFolder):\n",
        "    def __init__(self, root, transform=None, target_transform=None):\n",
        "        super(TripletImageDataset, self).__init__(root, transform, target_transform)\n",
        "        self.triplets = self.make_triplets()\n",
        "\n",
        "    def make_triplets(self):\n",
        "        triplets = []\n",
        "        for index, (img, label) in enumerate(self.samples):\n",
        "            anchor_idx = index\n",
        "            \n",
        "            for i in range(4):\n",
        "\n",
        "                positive_indexes = [ind for ind, (_, label_positive) in enumerate(self.samples) if label == label_positive and ind != index]\n",
        "                positive_idx = random.choice(positive_indexes)\n",
        "\n",
        "                negative_indexes = [ind for ind, (_, label_positive) in enumerate(self.samples) if label != label_positive]\n",
        "                negative_idx = random.choice(negative_indexes)\n",
        "\n",
        "                triplets.append((anchor_idx, positive_idx, negative_idx))\n",
        "        return triplets\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # print(index)\n",
        "        # print(self.samples)\n",
        "        # print(len(self.triplets))\n",
        "        # print(self.triplets)\n",
        "        anchor, positive, negative = self.triplets[index]\n",
        "        # print(anchor, positive, negative)\n",
        "        anchor_img, _ = super(TripletImageDataset, self).__getitem__(anchor)\n",
        "        positive_img, _ = super(TripletImageDataset, self).__getitem__(positive)\n",
        "        negative_img, _ = super(TripletImageDataset, self).__getitem__(negative)\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Grayscale(), transforms.Resize((224, 224)), transforms.RandomRotation((0, 350)), transforms.ToTensor()])\n",
        "# transform = transforms.Compose([transforms.Resize((224, 224)), transforms.RandomRotation((0, 350)), transforms.ToTensor()])\n",
        "\n",
        "dataset_train = TripletImageDataset('preprocessed/clahe/1st_session/raw_data/', transform=transform)\n",
        "dataset_test = ImageFolder('preprocessed/clahe/2nd_session/raw_data/', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=8, shuffle=True)\n",
        "\n",
        "num_classes = len(dataset_train.classes)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.005\n",
        "\n",
        "model = VGG16(num_classes).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adagrad(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def euclidean_distance(x, y):\n",
        "    \"\"\"\n",
        "    Compute Euclidean distance between two tensors.\n",
        "    \"\"\"\n",
        "    return torch.pow(x - y, 2).sum(dim=1)\n",
        "\n",
        "def compute_distance_matrix(anchor, positive, negative):\n",
        "    \"\"\"\n",
        "    Compute distance matrix between anchor, positive, and negative samples.\n",
        "    \"\"\"\n",
        "    distance_matrix = torch.zeros(anchor.size(0), 3)\n",
        "    distance_matrix[:, 0] = euclidean_distance(anchor, anchor)\n",
        "    distance_matrix[:, 1] = euclidean_distance(anchor, positive)\n",
        "    distance_matrix[:, 2] = euclidean_distance(anchor, negative)\n",
        "    return distance_matrix\n",
        "\n",
        "def batch_all_triplet_loss(anchor, positive, negative, margin=0.2):\n",
        "    \"\"\"\n",
        "    Compute triplet loss using the batch all strategy.\n",
        "    \"\"\"\n",
        "    distance_matrix = compute_distance_matrix(anchor, positive, negative)\n",
        "    loss = torch.max(torch.tensor(0.0), distance_matrix[:, 0] - distance_matrix[:, 1] + margin)\n",
        "    loss += torch.max(torch.tensor(0.0), distance_matrix[:, 0] - distance_matrix[:, 2] + margin)\n",
        "    return torch.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = F.pairwise_distance(anchor, positive, p=2)\n",
        "        distance_negative = F.pairwise_distance(anchor, negative, p=2)\n",
        "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
        "        return torch.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb Cell 14\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     negative_embed \u001b[39m=\u001b[39m model(negative)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     loss \u001b[39m=\u001b[39m triplet_loss(anchor_embed, positive_embed, negative_embed)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mmodel_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(epoch) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_array = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (anchor, positive, negative) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        anchor = anchor.to(device)\n",
        "        positive = positive.to(device)\n",
        "        negative = negative.to(device)\n",
        "        anchor_embed = model(anchor)\n",
        "        positive_embed = model(positive)\n",
        "        negative_embed = model(negative)\n",
        "        loss = batch_all_triplet_loss(anchor_embed, positive_embed, negative_embed)\n",
        "        loss_array.append(float(loss))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0 and epoch != 0:\n",
        "        torch.save(model.state_dict(), 'model_vgg_checkpoint_' + str(epoch) + '.pth')\n",
        "    print ('Epoch [{}/{}], Loss: {}' \n",
        "                   .format(epoch+1, num_epochs, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('model_vgg_checkpoint.pth', map_location=torch.device('cpu')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_train = ImageFolder('preprocessed/clahe/1st_session/raw_data/', transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data_map = []\n",
        "train_label_map = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in train_dataloader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images).detach().numpy()\n",
        "        train_data_map.append(outputs)\n",
        "        train_label_map.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "369"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data_map = []\n",
        "test_label_map = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_dataloader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images).detach().numpy()\n",
        "        test_data_map.append(outputs)\n",
        "        test_label_map.append(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 41, 269,  79, ..., 430, 208, 444])"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.concatenate(test_label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([187, 370, 273, 430, 219, 327, 295, 168]), tensor([226, 211,  20,  31, 226, 344, 213, 244]), tensor([ 40, 360, 191, 122, 165, 133, 407,  97]), tensor([103, 250,  75, 192,  63,  67, 372, 258]), tensor([307, 479, 227, 469,  87, 166, 337, 166]), tensor([375, 263, 121, 323, 425, 307, 406, 365]), tensor([113, 310,  18, 401, 326, 396,  55, 169]), tensor([402,  76, 242,  14, 368, 478, 208, 433]), tensor([190, 339, 129, 300,  64,  26, 455, 422]), tensor([398, 338,  28,  13,  37,  63, 165, 356]), tensor([323, 318,  32, 387,  71, 385, 265,  55]), tensor([105, 142, 402,  15, 252, 208,  12, 297]), tensor([483,  84, 289, 439, 207, 231, 161, 481]), tensor([213, 452, 396, 112, 262, 324,  78,   5]), tensor([264, 319, 193, 267, 209, 151, 248, 232]), tensor([451, 293, 457,  84, 325, 468, 198, 154]), tensor([376,  51,  32, 108, 473, 259, 179, 384]), tensor([236, 173, 136, 162, 394, 332, 400, 426]), tensor([182, 251, 306,  48, 487,  56, 221, 132]), tensor([386,  67, 265, 116, 388, 212,  57, 329]), tensor([477, 301, 424, 482, 406, 430, 274, 204]), tensor([405, 233,  92, 484, 322, 258, 116, 131]), tensor([216, 118,  90,  80, 413, 422, 369, 356]), tensor([204, 460, 208, 137, 226, 119, 119, 351]), tensor([126, 475,  22,  83, 141, 167, 310,  87]), tensor([378, 372, 403, 234, 485, 332,  72, 261]), tensor([194, 195, 288, 270,  45, 144, 202, 395]), tensor([474, 452, 348, 453, 228, 300, 384,  98]), tensor([ 46,  50, 119,  37, 130, 274,  40, 421]), tensor([ 48, 256, 192,   8, 475, 309, 265, 454]), tensor([161,  51, 394, 389, 475, 403,  59, 450]), tensor([363, 362, 234, 291, 225, 116, 434, 280]), tensor([360,   6, 466, 352, 308,  58, 219, 408]), tensor([ 42,  39, 354, 405, 155, 387, 457, 214]), tensor([  1, 229, 186, 449, 240, 150, 380, 183]), tensor([433, 183, 200, 420,  58, 407, 358, 480]), tensor([ 95,  70, 111, 316, 107, 451, 156,  86]), tensor([ 53, 110, 419, 412, 118, 428, 490,  65]), tensor([278, 137, 436, 120, 269,  95, 454, 367]), tensor([233,  11, 428, 264, 484,  22, 107, 376]), tensor([254, 472, 263, 329, 488,   6, 189,  32]), tensor([214, 285, 273, 253, 425, 190, 328, 174]), tensor([443, 235,  27, 361, 113, 303, 320, 281]), tensor([457, 138,  49, 233,  19, 263, 477,  12]), tensor([ 53, 355, 224, 230, 137, 348, 431, 388]), tensor([414, 288, 269, 369, 135, 267, 141, 102]), tensor([106, 320, 354, 347, 390, 409, 458, 319]), tensor([ 33,  46, 160, 452, 258, 467, 491, 383]), tensor([412,  72,  80, 154, 382, 400, 123, 133]), tensor([277, 158,   5, 244,  50, 198,  90, 246]), tensor([126, 219, 464,  13, 379, 183,  16, 237]), tensor([255,  98, 131, 147, 279, 283, 168, 472]), tensor([ 43, 164, 259, 324, 367,  40, 297, 465]), tensor([452,  62, 454,   1,  28, 299, 147, 200]), tensor([329, 355, 196, 238,   7, 313, 396, 403]), tensor([389, 111, 177, 349, 485, 377, 362, 363]), tensor([ 79, 334, 295, 281, 350, 396, 140,  15]), tensor([346, 162,   0, 115, 197, 408,  40, 102]), tensor([ 66, 152, 303,  18, 394, 361, 229,  63]), tensor([130, 406, 125, 397, 398, 335, 204,  50]), tensor([218, 350, 211, 411, 328, 198, 410, 109]), tensor([428,   3, 438, 230, 271, 134,  13, 456]), tensor([ 25, 139,  28, 404,  51, 363, 120,  67]), tensor([ 53, 290, 435, 150, 391, 468,   7, 122]), tensor([474, 336, 454, 406, 100, 443, 356, 178]), tensor([340, 266, 330, 331, 163, 299, 167, 192]), tensor([125, 145, 198, 296, 316, 487, 456,  59]), tensor([171, 258, 404, 214,  72, 237, 180, 124]), tensor([128, 144, 153, 127, 460,  74,  43, 198]), tensor([449, 128, 268, 235, 309,   0, 201, 103]), tensor([314, 224, 438, 280, 223,  28, 322,  14]), tensor([366,  60, 453, 484, 415, 266,  99,  39]), tensor([ 61, 242, 168, 462, 188,   9, 341,  17]), tensor([357, 320, 201, 203, 307, 321, 459, 216]), tensor([369, 345, 203, 202, 144, 366, 275, 486]), tensor([206, 344, 340, 116,  19, 167, 464, 179]), tensor([ 44, 458, 171, 393, 247, 330, 380, 464]), tensor([202, 232, 309, 345, 224, 403, 175, 198]), tensor([417, 402, 174, 390, 364, 327, 210, 226]), tensor([333, 284, 172,  25, 158, 258, 423, 387]), tensor([376, 470, 421, 270, 332, 417, 430,   4]), tensor([ 65, 154, 407, 381, 431, 480, 140, 416]), tensor([441, 254, 221, 149, 112, 215, 121, 173]), tensor([ 68, 432, 203, 312, 126, 391, 414, 246]), tensor([ 20, 432, 408, 267, 286, 270,  65, 320]), tensor([ 50, 377,  97, 394, 439,  76, 238, 314]), tensor([166, 159, 101,  37, 336,  35, 269, 306]), tensor([485, 415, 385, 271, 431,  91, 311, 111]), tensor([103, 483,  91, 318,  79,  82, 204,  60]), tensor([291, 297, 248, 448, 337, 245,  75, 453]), tensor([  2, 184, 240,  74,  98, 447, 140, 489]), tensor([157,  24, 260, 148, 103, 262, 229, 195]), tensor([ 96, 104, 199, 207,  59, 192,  41, 462]), tensor([368, 253, 459, 325, 359, 299,  25, 173]), tensor([348, 117,  15, 333, 456, 358, 199, 365]), tensor([441, 195, 335,  24, 184, 339, 443, 120]), tensor([155, 445, 277, 301, 235, 218, 290, 441]), tensor([220, 229, 284, 106, 255, 447, 327, 389]), tensor([369, 320, 103, 422, 330, 260,  19, 152]), tensor([308, 271, 344, 483,  96, 326, 418, 392]), tensor([119, 465, 293, 481, 324, 414,  93, 338]), tensor([193, 476,  31, 431, 366,  79, 188, 337]), tensor([147, 176,  10, 442, 448,  34, 241,  18]), tensor([  9, 370,  54,  66, 295, 409, 343,  31]), tensor([ 64, 170, 187, 313, 490,  93, 397, 145]), tensor([209,   4, 215, 164,  28, 193,  39, 372]), tensor([ 63,  22,   5, 151, 254, 241, 217, 134]), tensor([431, 452, 105, 385, 168,  84, 459, 455]), tensor([284, 101,  11, 433, 231, 453, 376,  42]), tensor([398, 246, 241, 468, 221, 474, 433, 302]), tensor([  6, 219, 314, 199, 464, 381, 275, 353]), tensor([424, 375, 108, 212, 377, 374, 163, 324]), tensor([257, 357, 435, 195, 251, 284, 313, 366]), tensor([462, 291, 294, 400, 378, 437, 237, 292]), tensor([476, 253, 207, 465, 339,  52,  61, 240]), tensor([142, 109, 186, 232, 476, 153, 383, 488]), tensor([478, 161, 327,   8,  11, 471, 396,  17]), tensor([298, 142, 287, 101, 305, 382, 273, 114]), tensor([ 16, 169,  93,  87, 325, 353, 480,   9]), tensor([287, 314,  38, 188, 171, 130, 167, 379]), tensor([211, 235, 181,  34, 440,  59, 305, 136]), tensor([131, 263, 390, 282, 249, 292,  68,  28]), tensor([393, 321, 428, 239, 375, 306, 179, 276]), tensor([381, 411, 488, 392, 323, 282,  83,  96]), tensor([312, 244, 473, 411, 226, 337, 350, 254]), tensor([303,  66, 163, 203, 426, 316, 268, 270]), tensor([ 85, 162, 356, 162, 103, 281, 356, 339]), tensor([352,  21, 289, 397, 307, 330,  36, 134]), tensor([427, 236, 450, 141, 227, 266,  30, 472]), tensor([303,  35,  62,   5, 207, 234, 178, 425]), tensor([172,  96,  41, 217, 143, 191, 253, 277]), tensor([413, 278, 420, 416,  29, 446, 187, 172]), tensor([ 62, 172, 421, 183,  32,  59, 239,   7]), tensor([ 74,  18, 342, 150, 354, 293, 439, 410]), tensor([ 76, 257, 334,  56, 175,  39, 234, 367]), tensor([351, 122, 379, 291,  55, 152, 358,  80]), tensor([135, 377,  18,  30,  17, 490, 304,  46]), tensor([ 82, 183, 318,   9, 307, 118, 136, 338]), tensor([413,   4, 213, 446, 354, 365,  94, 261]), tensor([125,  55, 405,  81, 462, 315, 253, 311]), tensor([164, 416, 323, 165, 280,  25,  49, 301]), tensor([178,  85, 255, 146, 427, 284, 305,  67]), tensor([353, 438, 399, 223, 278, 362, 443, 134]), tensor([293, 475,   2, 130, 361,  79, 249, 148]), tensor([115, 418, 481, 145, 242, 399, 399, 196]), tensor([271, 227, 212, 151, 346, 435, 140,  60]), tensor([419, 420, 491,  74,  76,  77, 372, 124]), tensor([488,  31, 360, 151, 136, 368, 459, 133]), tensor([101, 239, 346, 302,  91, 296,  57, 441]), tensor([344,  93, 207, 273, 377, 332, 269, 358]), tensor([119,  16,  69, 384, 214,  48,  44, 182]), tensor([463, 317, 460,  46, 395, 244, 442, 109]), tensor([104, 465, 448,  83, 375, 457,  37, 146]), tensor([405, 148, 156, 144, 476, 411, 189, 283]), tensor([328, 418, 208, 128,  81, 470, 276,  46]), tensor([393,  67, 105,  84, 436, 491, 407, 175]), tensor([157, 191, 348, 291,  17, 175,  36,  78]), tensor([154, 331, 245, 351, 261, 116,  26, 466]), tensor([283, 430,   1, 159, 157, 488, 238,  25]), tensor([153,  73,  58,  81, 472, 139, 300, 312]), tensor([347, 388, 176, 153, 180,  49, 245, 434]), tensor([184, 204, 174, 112, 282,  97, 212, 101]), tensor([398, 391,  98, 381, 401, 370, 181,  92]), tensor([186, 334, 200, 407, 163, 483, 213, 220]), tensor([338, 365, 377, 247, 200, 133, 358, 107]), tensor([285, 485, 311, 392, 403, 243, 390, 206]), tensor([376, 170, 114, 374, 399, 196, 296, 159]), tensor([208, 440, 104, 120, 241, 235, 243, 423]), tensor([448, 318,  42, 256,  48,  36, 273, 113]), tensor([378, 441,   3, 438, 395, 170, 342, 379]), tensor([446, 330, 232,  46,  55, 173, 131, 484]), tensor([134,  20, 290, 217,  77,  49, 461, 254]), tensor([272, 367, 484,  10,  91, 345, 336,   4]), tensor([466, 348, 168, 347, 240,  78, 128, 209]), tensor([442, 246, 384, 412, 359,  89, 114, 249]), tensor([283, 285, 244, 197,  21,  82, 276, 252]), tensor([268, 248,  11,  60, 164, 391,  80, 489]), tensor([ 80, 192, 176, 389, 275, 124,  47, 470]), tensor([386, 227, 459, 483, 451, 382, 481, 454]), tensor([205, 429, 488, 262, 304, 231, 129,  56]), tensor([114, 155, 395,  77, 437, 463, 166, 265]), tensor([450, 468, 184, 274, 171, 117, 464, 202]), tensor([326, 142,  70, 191, 193, 486,  66,  94]), tensor([ 56, 216, 418, 194,  32, 289, 444, 197]), tensor([ 72, 221, 327,  14, 138, 247, 205,  51]), tensor([257, 330,  16,  21, 325,  35, 218,  43]), tensor([386, 255, 189, 317,  30, 237,  78,  45]), tensor([ 59, 372, 231, 332, 207, 401,  86, 245]), tensor([354, 339, 446, 310, 162, 423, 383, 199]), tensor([146, 127, 280, 382, 138, 210,  12,  54]), tensor([196, 417, 182, 138, 173,  52,  75, 397]), tensor([322, 333,  38, 243, 393, 105,  41, 223]), tensor([402, 483, 152, 302, 298, 317, 399, 170]), tensor([200, 191, 390, 177,  60, 277, 222, 236]), tensor([146, 449, 473, 374, 463, 166, 105,  96]), tensor([122,   7, 149, 381, 201, 117, 167, 100]), tensor([349, 232,  64, 237,  94, 345, 406, 206]), tensor([259, 287, 282, 298, 100,  95, 458, 136]), tensor([ 71,  69, 346, 321,  97, 272, 436, 192]), tensor([188, 477, 225, 246, 370, 445, 169,  48]), tensor([355, 456,  64, 363,   6, 385, 104, 348]), tensor([346, 248, 155, 401, 275, 478, 362,  70]), tensor([309,   7, 427, 310, 310, 317, 272, 145]), tensor([479, 480,  17, 457,  70, 194, 239,  54]), tensor([311, 426, 343,  51, 296, 388,  22,  83]), tensor([ 95, 380,   2, 106,   9, 124, 315,  82]), tensor([341, 128, 135, 262,  94, 180, 193, 387]), tensor([179, 383, 476, 435, 380, 215, 260,  49]), tensor([123, 467, 227, 462,  29, 115, 203, 484]), tensor([351,   2, 183, 292, 233, 370, 121, 297]), tensor([137, 115,  88, 283, 380, 340, 335, 187]), tensor([ 90, 216, 305, 141, 266, 197,  42, 436]), tensor([303, 187, 374, 152,  29, 159, 201, 355]), tensor([402,  47,  26, 229, 321, 489, 481, 303]), tensor([200, 153, 248, 127,  71,  33, 297, 450]), tensor([386, 304, 473, 349, 176, 453,  35, 373]), tensor([313, 418, 302, 217, 182, 276,  53, 415]), tensor([445, 272, 120, 162,  99, 442, 230,  69]), tensor([368, 231,  22, 182, 104, 154, 394, 281]), tensor([451, 434, 229, 126, 480, 164, 210,  33]), tensor([263,  15,  80,  43, 426, 112, 489, 242]), tensor([160, 422, 186,  88, 299, 357, 467, 250]), tensor([240, 174, 181,  96, 423, 467, 143, 370]), tensor([  1, 270, 160, 369, 286, 472, 467, 372]), tensor([  8, 156, 237,  35,  66, 360, 174,  75]), tensor([116,  34, 153, 428, 107,  60, 109, 375]), tensor([313, 357,   2, 333, 206, 274, 474,  19]), tensor([141, 480, 423, 420, 140, 255, 398,  39]), tensor([250, 267, 318, 194, 250,  38, 320, 190]), tensor([461, 324, 465, 322, 336, 221, 291, 447]), tensor([ 23,  90, 264, 248, 312,  26, 209, 319]), tensor([ 15,  47, 301, 396, 416,  29, 133,  73]), tensor([414, 409, 143, 408, 420,  44, 181, 466]), tensor([ 12, 383,  36, 282, 304, 252, 127, 490]), tensor([424, 238, 380, 349,  75, 289, 342,  72]), tensor([144, 378, 340, 178,  14, 177, 165, 138]), tensor([282, 457, 455, 342,  86, 171, 373, 214]), tensor([ 54, 194,  37,  76, 120,  70, 133,  68]), tensor([148, 309, 408, 400,  87, 306, 365,  45]), tensor([ 22, 147, 298, 331,  47,  58, 178, 102]), tensor([ 49, 473, 101,  61, 322,  12,  38, 298]), tensor([113, 202,  21, 215,  34, 311, 126, 138]), tensor([ 69, 296,  77, 238, 167, 178, 367,  62]), tensor([206, 472,  76, 464,   0, 325, 350, 301]), tensor([161,  40, 482, 257, 344, 410,  13,   3]), tensor([ 91, 279,  52, 482, 224, 364, 368, 225]), tensor([413, 439, 211, 107, 156, 305,  52,  58]), tensor([357, 213, 321,  20,  72, 146, 443, 259]), tensor([482,  57,  52,  19,  90, 236, 437, 313]), tensor([ 99, 174, 353, 294, 125, 487, 109, 292]), tensor([122, 205, 140, 335, 286,  83, 343, 224]), tensor([439,   2, 387, 290, 351,  69, 412, 482]), tensor([ 14, 145, 148, 268, 211, 165, 315, 227]), tensor([230, 419, 420, 471, 143, 465, 151, 414]), tensor([261, 287, 235, 161,  20,  64, 386, 341]), tensor([266, 419, 433, 410, 123, 111, 139, 252]), tensor([334,  61, 478, 404, 137, 240, 111, 475]), tensor([100, 466, 247, 452, 442, 117, 429, 460]), tensor([257,  61,  89,   3,  16, 289, 364,  23]), tensor([474, 195, 251, 474, 149, 407, 440, 477]), tensor([468, 491, 315, 404,  35,  85, 394, 209]), tensor([262, 426, 157, 221, 462, 309, 163, 218]), tensor([292, 306, 369, 392, 189, 383, 252, 295]), tensor([316, 285, 300, 163, 129,  75, 217, 249]), tensor([ 37, 238, 405,  94,  47, 458,  99, 288]), tensor([ 56, 261, 134, 185,  89,  13, 358, 175]), tensor([463,  77,  30, 379,  99,  86, 293,  71]), tensor([308, 261, 463, 117, 487, 331, 245,  92]), tensor([416, 446,  26, 294,  91,  41, 180, 315]), tensor([147, 218, 139, 424, 448, 118,  81, 246]), tensor([223, 359, 171,  14, 337, 172, 447, 155]), tensor([271, 334, 210, 212, 185, 121, 188, 263]), tensor([106, 142,  23,  40, 143, 471, 264, 168]), tensor([ 27, 454, 335, 373, 219, 278, 113, 195]), tensor([395,   1, 304, 264, 121, 110, 321, 247]), tensor([325, 179, 170, 460, 326, 106, 108, 132]), tensor([319,  69,  30, 371,  74, 197, 149, 434]), tensor([ 24, 425, 299, 360, 373, 139, 489, 352]), tensor([ 84, 156,  98,   4, 279, 280, 389, 448]), tensor([208, 108, 432, 109,  56, 445,  65,  92]), tensor([359, 478, 149, 455, 172, 421, 451, 190]), tensor([364, 453, 479, 222, 266, 441, 132, 401]), tensor([400, 468,  41, 239, 397, 179, 300, 305]), tensor([415, 361, 392, 125, 290,   3, 247, 146]), tensor([ 43, 415, 334, 160, 233, 166, 338, 442]), tensor([215,  73, 367, 363, 115,  99, 203, 228]), tensor([469, 139, 364,  82, 371, 262, 373,  79]), tensor([485, 323, 485, 486, 324, 389, 114, 301]), tensor([136, 113,   1,  45, 339, 487, 220, 444]), tensor([205, 469, 234, 378, 255,  38, 251, 267]), tensor([223, 244,  92, 426, 160, 302, 440, 363]), tensor([ 21, 436, 327, 362, 479, 379, 213, 361]), tensor([ 83, 347, 430, 446, 311, 149,  27, 281]), tensor([ 70, 131, 428, 124, 437, 129, 170, 341]), tensor([393, 286, 408, 176, 164, 100, 423, 365]), tensor([ 64, 236,   0, 181,  50, 491, 256, 345]), tensor([326,  31,  73, 399, 143, 197, 342, 443]), tensor([ 39, 385, 331, 279, 362, 184, 456,  65]), tensor([206, 435,  87,  20, 228, 414, 425, 290]), tensor([316, 436, 258, 252, 165, 169, 467,  97]), tensor([269, 449, 110, 455, 346, 444, 466,  42]), tensor([338, 216, 421,  54, 419, 329, 181, 123]), tensor([ 44, 335, 242, 288, 287, 225,  48, 233]), tensor([302,  34, 343, 100, 135, 185, 182, 438]), tensor([210,  58, 102, 269, 344, 416, 393, 286]), tensor([486,  88, 447, 419, 111,  24, 345, 429]), tensor([184, 132, 350, 333, 265,  41,  36,  68]), tensor([169, 249, 115,  27,  74, 289, 243, 257]), tensor([304, 245, 437, 220, 410, 260, 477, 382]), tensor([157,  11,  10,   7, 459, 450, 471, 219]), tensor([440, 438,  78, 279, 214, 470, 159, 228]), tensor([176, 250, 185, 427,  88, 127, 331, 102]), tensor([169, 447,  88, 220, 349, 314, 435, 201]), tensor([150,   5, 180, 371, 216, 470,  17,  30]), tensor([429, 308, 158, 397, 432, 277, 352, 315]), tensor([  4, 190, 332,  57, 374, 226, 196, 173]), tensor([ 85, 388, 268,   6,  57, 142, 281, 298]), tensor([328, 199, 177, 384, 144, 406, 188, 392]), tensor([ 65,  71,  12,  10,  51, 385, 424, 265]), tensor([268, 205, 404, 177,  33, 267, 449, 295]), tensor([  9, 440, 243, 427, 186, 357, 490, 450]), tensor([326,  68, 458, 288, 123, 212, 415,  19]), tensor([ 79, 317, 486, 234, 336,  10,  67,  27]), tensor([ 78, 285, 390, 373, 337, 294, 202, 413]), tensor([275, 275,  66, 156, 222,   0, 352, 312]), tensor([119, 132, 314,  71, 158,   8, 129, 432]), tensor([482, 341, 199, 150, 463, 189,  85, 356]), tensor([461, 308, 403, 135, 278, 280,  61,  97]), tensor([410, 102, 300,  95, 222,   3,  87, 477]), tensor([ 95, 236, 287, 430, 108, 355, 306, 222]), tensor([417, 471, 400,  32, 461, 135, 118, 476]), tensor([ 53,  23, 374, 256, 366, 264,  27, 455]), tensor([469, 359, 201, 323,  54, 222, 285, 329]), tensor([ 62, 417, 228,  18, 250,  34, 249, 409]), tensor([ 52, 158, 333,  31, 322, 458, 319, 273]), tensor([307,  29, 409, 384, 424, 469, 185, 215]), tensor([378, 340,  23, 411, 145, 444, 251, 368]), tensor([359,  26, 299, 224, 185,  68, 293, 114]), tensor([350, 241,  21, 308, 351, 471,  33, 422]), tensor([211, 361, 232, 328, 243, 489,  55, 353]), tensor([189, 354, 186, 469, 460, 387, 148, 160]), tensor([ 81, 434, 412, 439, 152,  16,  89, 276]), tensor([425,  73, 260, 395, 147,   6, 422, 175]), tensor([310, 230, 347,  82,   8, 371, 283, 123]), tensor([137, 157,  24, 122, 376, 490,  77, 277]), tensor([413, 127, 286,  13, 106, 284, 118, 451]), tensor([360, 418,  45, 481,  93, 318, 279, 491]), tensor([272, 194, 470, 312, 355,  24, 295, 391]), tensor([353, 132, 429,  86, 272, 402, 343, 129]), tensor([292, 110, 159, 231, 316, 456, 371, 479]), tensor([126, 382, 230, 242, 297, 141, 228, 364]), tensor([ 44, 210, 401, 294, 473, 193,  53, 274]), tensor([ 10, 342, 434, 158, 155, 444, 336, 124]), tensor([ 88,  81, 223, 398, 105,  89, 104, 479]), tensor([187, 191, 340, 475,  38, 259, 256, 270]), tensor([444, 130,  85,  47, 391, 449, 131,  86]), tensor([ 33, 341,  73, 209, 253, 241, 256, 329]), tensor([294, 461, 225, 366, 429, 225,   0,  89]), tensor([427,  94, 421, 432, 319, 486, 108, 388]), tensor([409, 110, 437, 445, 150,  43, 478, 278]), tensor([254, 217, 260,   8, 177, 151,  25, 371]), tensor([ 63, 121, 343,  36, 251,  29, 349, 417]), tensor([ 92, 405, 161,  42, 112,  98, 433, 259]), tensor([487, 205,   5,  57, 317, 125,  63, 196]), tensor([ 62, 431, 296, 461, 445, 386,  50, 117]), tensor([347, 239, 412,  23, 274,  90,  44, 276]), tensor([154, 180, 352, 328,  45, 218, 204, 271]), tensor([220, 411, 107, 381, 288,  11, 404,  84]), tensor([128, 375,  15, 130, 110, 112,  93, 190])]\n",
            "[tensor([ 41, 269,  79, 387, 437, 200, 216, 231]), tensor([306, 366, 359, 385, 488, 124, 131, 107]), tensor([288, 305, 485,   6, 245, 388, 312, 456]), tensor([435, 242, 275, 262, 330,  70, 438, 290]), tensor([ 86, 244, 437,  17, 302, 487, 157,  54]), tensor([413, 450, 266,  52, 457,  81, 304, 145]), tensor([147,  95, 381, 283, 177, 151, 356, 263]), tensor([149,  99,  22, 246, 478, 306, 390, 488]), tensor([163, 328,  20, 392, 284, 115, 131, 417]), tensor([165, 213, 201, 166, 445, 148, 164, 131]), tensor([ 58, 373, 415, 328,  76, 247,  56, 430]), tensor([352,  31, 382,  86,  26,  92, 462,  32]), tensor([123, 272, 132,  11, 366,  46, 434,  63]), tensor([299, 393, 135, 358, 410, 362, 451,  29]), tensor([ 62,  98, 389, 426,  13, 356, 273, 213]), tensor([  1, 155, 105,  96,  97, 350, 216, 117]), tensor([149,  69, 439, 400, 136, 358, 385, 259]), tensor([190, 257, 337, 380, 186, 360, 120,  11]), tensor([160, 367, 476, 389, 316, 102, 345, 366]), tensor([125,  47, 317, 110, 112, 291, 206, 198]), tensor([ 94, 342, 315,  57, 134, 394, 250, 217]), tensor([219, 459, 353, 398,  47, 265, 153, 363]), tensor([ 10, 336, 179, 445, 131, 357, 361, 272]), tensor([477, 245, 392, 395, 183,  26, 255, 397]), tensor([246, 347, 147, 330, 251, 267, 204,  23]), tensor([110, 410, 348, 331,  29, 371, 256, 349]), tensor([ 78, 455, 484, 327,  65,  88, 153,  63]), tensor([313, 274, 361, 462,  49, 344,  97, 272]), tensor([477, 404, 366,   2,  21, 480, 309,  52]), tensor([ 67, 183, 421, 271, 384, 365, 164, 427]), tensor([449,  38, 118, 346, 339, 129,  84, 450]), tensor([ 20, 431, 135, 109, 362, 141, 299, 433]), tensor([325, 261, 420,  76,  34, 292, 104, 447]), tensor([310, 359, 311,  32, 379, 274, 465,  68]), tensor([ 78, 321, 253, 346, 240, 268,  94, 113]), tensor([177,  28,  87, 293, 477, 147, 449,  86]), tensor([354,  62, 241,   0,  85, 406,  77, 426]), tensor([350, 128,  92,  38, 425, 427, 440, 192]), tensor([141, 398,   3, 458, 183, 475, 365, 334]), tensor([347, 432,  50, 446, 105, 108, 269, 272]), tensor([ 74,  37,  93, 239, 191, 227, 277, 233]), tensor([ 64, 303, 402, 211, 230, 394, 381, 207]), tensor([462, 326,  62, 336, 154, 468, 405, 347]), tensor([491, 195, 318, 467, 169,  50,  54,  16]), tensor([ 85, 146, 255,  30, 127,  68, 248,  61]), tensor([472, 399,  94, 268, 203, 193, 162, 331]), tensor([109, 377, 357, 197, 100, 325, 218, 168]), tensor([ 56, 163, 210, 273, 230, 101, 412, 309]), tensor([213, 229, 451, 314, 442, 196, 184, 290]), tensor([487, 420, 488,   6, 345, 241,  78, 134]), tensor([450, 148, 421, 154,  68, 310, 240, 445]), tensor([445, 408, 385, 132, 370, 155, 460, 127]), tensor([181,  98, 186, 107, 121, 356,  88,  64]), tensor([  7, 152, 280, 285, 352, 464, 372, 238]), tensor([ 22,  25, 401, 243, 469, 188, 486, 200]), tensor([427,  15, 365, 103, 305,  52, 188,   1]), tensor([491, 321, 236, 453, 238, 163, 127, 403]), tensor([285, 370, 386, 253, 405, 405, 219, 358]), tensor([316, 286, 415,  28,  14, 232, 314,  79]), tensor([ 20, 232,  72, 196,  69, 292, 273, 181]), tensor([ 48,  37,  42, 160, 191, 443, 451, 215]), tensor([462,  65, 204,  58, 369,  61, 188, 355]), tensor([429, 235, 412, 133, 472, 221, 422, 252]), tensor([174,  86, 292,  64,  83, 138, 466, 126]), tensor([167, 297, 189, 491, 227,  37, 168,  30]), tensor([287, 344, 308, 384, 165, 166,  90, 267]), tensor([ 73, 191, 212, 465,  80, 457, 189, 136]), tensor([379, 224, 102, 321,  73, 462, 376, 300]), tensor([ 80, 382, 125, 327, 170, 488, 338,  36]), tensor([425, 155, 454, 481, 214, 188, 247, 485]), tensor([481, 414, 239, 247, 132, 249, 230, 283]), tensor([149, 362, 396, 416, 388, 184, 204, 311]), tensor([ 13, 157, 139, 255, 360, 338, 220, 313]), tensor([ 36, 317, 411, 123, 186,  75, 334, 201]), tensor([416, 345,   9, 357, 471, 312, 164, 386]), tensor([425, 314, 394, 229,  87,  47, 231, 351]), tensor([399, 336, 283, 309, 416,  77, 344, 343]), tensor([ 78, 398, 279, 482, 309, 144, 193, 150]), tensor([ 24,  45, 122, 402, 321,  23, 204, 381]), tensor([380,  72, 108,  42, 279,  30, 254, 444]), tensor([416, 102, 331, 279, 239,  90, 340,  83]), tensor([203, 317, 391, 231, 312, 269, 206, 377]), tensor([ 42, 144, 184,  70,  95, 330, 104, 313]), tensor([ 72, 245, 450, 220, 140,  46, 115,  45]), tensor([250, 285,  74, 346, 236, 196, 382, 327]), tensor([351, 105, 161, 137,   6, 487, 455, 269]), tensor([414, 421, 118, 393, 363,  51,  74, 130]), tensor([ 64, 172, 313, 444, 233, 198,  59, 305]), tensor([  2, 380, 476,  60, 236, 165,  92, 399]), tensor([163, 315,  19, 318,   4, 381, 185,  24]), tensor([307,   8, 377, 122, 183, 481, 167, 304]), tensor([299, 406,   6,  24, 229, 449, 483, 295]), tensor([225, 146,  32, 430,  78,  39, 295,   4]), tensor([182,  60, 275,  36, 294, 169, 383, 469]), tensor([ 31,  46, 112, 122, 237,  44,  22, 440]), tensor([198, 387,  40,   2, 317, 421, 439, 212]), tensor([442, 375, 178, 461,  71, 456, 150, 231]), tensor([144, 467,  89, 422, 402,  17,  60, 103]), tensor([440, 269,  13, 434,  46, 446, 437, 122]), tensor([315, 488, 142, 303,  82, 335,  75, 341]), tensor([161,  29, 319, 163, 447, 142, 110,  49]), tensor([ 31, 438, 419, 440, 368, 235,  65, 182]), tensor([160, 412, 399, 112, 316, 432, 205, 234]), tensor([ 65, 434, 390, 192, 372, 388,  91,  82]), tensor([448, 171, 223, 474, 365, 437,  71, 419]), tensor([355, 296, 465, 328, 418, 195, 438, 446]), tensor([207,  81, 275, 237, 114,  31, 424, 486]), tensor([ 88, 239, 137, 344, 243, 441,  79, 384]), tensor([180, 443, 215, 281, 235,   8, 303, 478]), tensor([264,  49, 123, 469, 418,  75, 116, 345]), tensor([442, 423, 211,  90, 175, 454, 105, 130]), tensor([221, 386, 451, 232, 484, 100,   6, 145]), tensor([318, 186, 256, 477,  19, 340, 125, 466]), tensor([243, 265, 273, 275, 350, 239, 425, 491]), tensor([262, 222, 487, 124, 293,  66, 128, 126]), tensor([349, 108, 211,  25, 382, 257,  16, 332]), tensor([343, 171, 408, 424, 193, 473, 242,  26]), tensor([ 43, 434,  61,  11,  58, 108, 164, 455]), tensor([140, 471,  53, 222, 335,  86, 103, 412]), tensor([390, 268, 466,  21, 306, 468, 466, 391]), tensor([398, 259, 202, 473, 430, 334, 474, 375]), tensor([ 80,  14, 176, 378, 409, 464, 287, 439]), tensor([129, 124,  45,  89,  63, 348, 147, 166]), tensor([135, 367, 359,  56, 447,   5, 181,  91]), tensor([437, 100, 183, 302,  47,  51, 329, 361]), tensor([268, 305, 409, 482, 422, 245, 481, 303]), tensor([ 66, 303, 383, 190, 362, 116, 480, 423]), tensor([333,  95, 264, 389, 138, 321, 353, 315]), tensor([ 34, 395,  14,  40, 265, 329, 117, 441]), tensor([ 48, 287, 300,  67, 377, 155,  96, 282]), tensor([294,  76, 419,  98, 309,  53, 195, 173]), tensor([432,  61, 261, 426, 271,  10,  47, 374]), tensor([197, 406, 351, 114, 357, 487, 485, 110]), tensor([312, 417, 222, 128, 272, 205, 119, 304]), tensor([368, 196, 385, 403, 174, 409, 224, 380]), tensor([160, 248,  18, 291, 355, 185, 341, 383]), tensor([293, 472, 290, 226, 422, 222, 151, 150]), tensor([179, 221, 105, 483, 238, 367, 344, 211]), tensor([340, 257, 363, 101, 360, 368, 187,  42]), tensor([ 16, 411, 192,  72, 401, 274,  28, 314]), tensor([441, 187, 288, 305, 342, 138, 414, 182]), tensor([205, 102, 293, 362, 339, 111, 489, 346]), tensor([184, 170,  67, 478, 373, 358, 147, 210]), tensor([304, 204,   4, 172,  97, 281,  15, 128]), tensor([430, 146,   9, 109, 333, 472, 289, 431]), tensor([378, 457, 479, 218, 136, 429, 322, 146]), tensor([410, 400,  93, 404, 242, 461, 349,  40]), tensor([459, 251,  49,  23, 394, 227,  38, 356]), tensor([281, 281, 278, 302, 156, 471, 185, 461]), tensor([ 99, 259, 253,  71, 288, 469, 209, 460]), tensor([300,  20,   1, 229, 333, 275,  18, 142]), tensor([288,   7, 151, 113, 191, 307, 324, 351]), tensor([395, 322, 303, 339, 140, 227, 260, 438]), tensor([ 83,   5, 159, 475, 241, 313, 399, 327]), tensor([212,  81,  24, 453, 229, 179, 118, 476]), tensor([ 57, 114, 450, 450, 141, 478,  52, 296]), tensor([291, 387, 191,  69, 246, 252, 259, 323]), tensor([328,  39, 189,  73, 490, 237, 106, 417]), tensor([365, 246,  12, 400, 319, 189, 420, 249]), tensor([175, 327,  40, 157, 266, 379, 273, 195]), tensor([310,   0,  75, 226, 300, 154, 447, 292]), tensor([143, 286, 435, 258,  85, 201,  93, 245]), tensor([235,  87, 209, 471, 154, 458, 215, 157]), tensor([317, 463,  38,  91,  56, 243, 407, 449]), tensor([280,  87, 336, 373,  62, 142,  15, 267]), tensor([480, 157, 446, 457, 199, 360, 266, 154]), tensor([472, 177, 264, 211, 398, 216, 246, 415]), tensor([383, 277,   5, 114, 323, 312, 418, 119]), tensor([474,  70,  69,  57, 371, 176, 468, 446]), tensor([255, 322, 428, 391,  91, 202, 194, 224]), tensor([335, 310, 361, 261, 107, 152, 145, 126]), tensor([463, 234, 369,   9, 185, 254, 219,  43]), tensor([ 34,  22,  19, 455, 332, 295, 384, 436]), tensor([326,  25,  12, 316, 312, 114, 421, 301]), tensor([353, 331, 111, 356, 427, 170, 379, 127]), tensor([ 61, 128, 440, 442, 176, 460, 185, 311]), tensor([276, 329, 428,  29,  21, 286,  33, 424]), tensor([476, 443,  41, 226, 466,  17, 116, 192]), tensor([172, 130, 170, 490, 349, 418, 290,  22]), tensor([ 64, 436, 118, 286, 220,  29, 300,  60]), tensor([353,  36,  31, 376, 319, 207, 126, 109]), tensor([284, 408,   8, 270, 480,  52,  37, 161]), tensor([148,  68, 476, 469,  55, 131,  35, 131]), tensor([218, 424, 121, 323, 112,  89,  21, 410]), tensor([480,   4, 254, 274,  87, 299, 174, 381]), tensor([137, 207, 337, 380, 149, 263, 278, 471]), tensor([144, 433, 285, 214,  65,  57,  48, 159]), tensor([139,   8,  48, 149,  84, 313,  32, 352]), tensor([485, 178, 268, 123,  27, 248,  72, 442]), tensor([238, 162, 390, 173,  28, 482, 133, 292]), tensor([ 45, 280, 156, 172,  87, 201,  84, 393]), tensor([ 60,  43, 206, 324, 175,   3, 461, 173]), tensor([428,  88, 409, 472, 422,  56, 287, 142]), tensor([369, 104,  96, 177,   4, 226,  10, 168]), tensor([463, 443,  81, 102,  51,  36, 364, 375]), tensor([ 80, 237, 187,  14,  12, 326,  82, 318]), tensor([ 94, 244, 119, 150,  25,  83, 347, 359]), tensor([252, 486, 294, 481, 229,  84, 139, 218]), tensor([214, 388, 332, 309, 292, 368, 115, 153]), tensor([297, 274, 289, 480, 448, 233, 334, 225]), tensor([426, 204, 473, 234, 444, 162, 459,  62]), tensor([135, 219, 491, 137, 180, 390, 173, 135]), tensor([228, 245, 212,  77, 201, 195, 311, 387]), tensor([233,  92, 308, 308, 273, 125, 192, 214]), tensor([228, 169,  41, 484, 237, 451, 153, 301]), tensor([459, 487,  21, 283, 485, 159, 325, 415]), tensor([410, 489, 448, 453, 306, 175,  70, 219]), tensor([427,  26, 483, 364, 484,  73, 448,   2]), tensor([136,   2, 153, 490, 337, 404, 317, 261]), tensor([378, 248, 271, 278, 199, 194,  27, 106]), tensor([265, 404, 338,  35, 478, 138, 140, 320]), tensor([128, 282, 429,  91, 254, 369, 277, 152]), tensor([121, 106, 347, 250, 461, 182, 364, 411]), tensor([306, 236, 249, 279, 271, 284, 383,  54]), tensor([276, 159, 107, 130, 408, 323, 151, 173]), tensor([419, 341, 307, 470, 405, 458,  39, 231]), tensor([137, 439,  11,  33, 395, 217, 331, 184]), tensor([416, 488,  25, 308, 206,  10,  55,  61]), tensor([435, 470, 244, 218, 423, 396, 345, 158]), tensor([252, 119, 251, 294, 113, 470,  70, 406]), tensor([432, 133, 402,  73, 275,  50, 468,  42]), tensor([248, 290, 114, 223, 489, 325, 302, 113]), tensor([396, 333, 267, 360, 278,  32,  18, 357]), tensor([250,  23, 452, 407, 104, 230, 165, 148]), tensor([168, 162, 132,  17, 326, 210, 158, 464]), tensor([479,  85,  83,  53, 283, 407, 389,   3]), tensor([146, 250, 423, 165, 298, 358, 195, 382]), tensor([264, 196, 449, 262, 375,  49, 447,  39]), tensor([379, 111, 148, 174,  81, 226, 230, 367]), tensor([305,  82,  23, 222,  35, 261, 134, 123]), tensor([121, 369, 465,  10, 339,  16, 310, 285]), tensor([247, 121, 483, 267, 187, 177, 470,  43]), tensor([335, 324, 297, 110, 117, 242, 307, 158]), tensor([489, 103, 262, 319, 296, 197, 289, 248]), tensor([354, 260, 145, 357, 181, 144, 373,  88]), tensor([386, 203, 281,  98, 261, 174, 254, 249]), tensor([342, 448, 113,  46, 143, 106, 172, 445]), tensor([351, 390, 134,  83, 296, 210, 435,  69]), tensor([276, 482,  85,  67, 456, 323, 431, 411]), tensor([289, 274, 405, 139, 129, 134, 209, 246]), tensor([  4, 310,  30, 417, 161,   2, 227, 176]), tensor([143, 358, 270,  69,  62, 483, 365,  63]), tensor([118, 123, 282, 126, 278, 202,   0, 223]), tensor([414,  86, 158, 252, 238, 388, 438, 320]), tensor([236, 435,   8,  95, 407,  27, 371, 406]), tensor([258, 346,  68, 367, 235, 431,   0, 171]), tensor([371, 249, 107, 136, 374, 403,  12, 215]), tensor([199, 166, 452, 228,  33, 475, 291, 420]), tensor([391, 115, 436, 126,  35, 241, 212, 326]), tensor([454, 130, 336, 216, 458, 413, 156, 296]), tensor([151, 152,  55, 418, 431, 414,  21, 207]), tensor([217, 271, 260, 348, 152, 191,  94, 258]), tensor([464, 139, 349, 262, 247, 453,   0, 384]), tensor([159, 228, 155, 354, 167, 117, 145, 350]), tensor([209,  44,  92, 343,  11, 422,  24, 226]), tensor([ 37,  95, 297, 356,  71, 328,   7, 179]), tensor([346, 117, 464, 468, 205,  90, 429,   9]), tensor([ 78, 162, 178, 475, 324, 160, 251, 372]), tensor([225, 397, 444, 473, 408, 337, 173, 364]), tensor([167, 462, 412,  53, 142, 441,  37, 413]), tensor([244, 260,   3,  12, 200, 419, 311, 367]), tensor([237, 433,   6, 279, 373, 412, 335, 467]), tensor([206, 315, 170, 484, 490,  59, 215,  17]), tensor([224, 378, 144, 341, 127, 460, 216,  99]), tensor([419, 431, 109,  48, 216, 403, 376, 283]), tensor([ 77,  59, 470, 150,  15,  20, 215, 403]), tensor([138, 490,  79,  96, 282, 258, 194, 401]), tensor([ 95, 474, 174, 407,  74, 103,  77,  35]), tensor([232, 355, 452, 385, 347, 298, 446, 441]), tensor([354, 228, 190, 225,  53, 250, 396,  97]), tensor([222, 349,  88, 451, 479, 288, 168, 193]), tensor([372, 176,  46, 448, 198, 291, 280, 243]), tensor([314, 398, 336, 417, 386, 335,   8, 241]), tensor([284, 232, 297, 112, 214, 196, 124, 421]), tensor([ 51, 489,  80, 467, 368,  38,  65, 343]), tensor([125, 453,  13, 192,  15, 301, 372, 127]), tensor([468, 296, 387, 298,  55, 180, 205, 343]), tensor([ 18, 326, 457,  19, 475, 156, 210, 473]), tensor([287, 118, 290,  77, 416, 370,  45, 301]), tensor([143, 340,  75, 202, 135, 101, 467,   1]), tensor([374,  27,  76, 255,  71, 395, 375, 132]), tensor([352, 228, 266, 371, 486,  94, 403, 341]), tensor([137,  44,  57,  41, 378, 161,  51,  54]), tensor([318, 153, 393, 279, 159, 141, 333, 206]), tensor([286, 180, 289,   3, 482, 393, 474, 125]), tensor([ 30, 190, 350,  79, 212, 115, 334, 280]), tensor([200, 236, 113, 409,  99, 366, 338, 458]), tensor([443, 467,  35, 401, 396, 381, 217, 374]), tensor([210,   5, 129, 370,  85, 213, 377, 453]), tensor([119, 428, 120,  17, 440,   3, 382, 209]), tensor([355, 433,  43, 270,  14, 387, 465, 478]), tensor([247, 146, 130, 256, 445, 253, 413, 200]), tensor([400, 320, 284, 374, 402, 270, 350, 394]), tensor([102,  90, 424, 258, 178,  59,   7, 471]), tensor([298, 221, 244,  89, 136, 406, 197, 208]), tensor([465, 234, 256, 329, 456, 175, 459, 107]), tensor([361, 392, 240, 106,   1, 156, 442,  59]), tensor([109, 363, 391, 424,  33,  99, 218, 223]), tensor([486, 269, 464, 363,  93, 397, 432, 138]), tensor([481, 197,  84, 106, 140,  43, 417, 315]), tensor([133, 240, 194,  58, 190, 265, 370, 342]), tensor([455,  20, 366,  75, 318, 251, 167, 463]), tensor([ 66, 270, 483, 147,  66, 397, 199, 262]), tensor([473, 477, 217, 257, 345, 325, 404, 319]), tensor([ 74, 402,  26, 263, 120,  34, 460, 189]), tensor([213,  97, 234, 460, 343,  54, 302,  66]), tensor([287, 319, 100, 277, 165,   7, 145, 436]), tensor([194, 264,  74, 304, 101, 308, 264, 124]), tensor([322, 208, 316, 393,  33, 294, 263, 438]), tensor([479, 122, 354, 105,  96, 168,  52,  50]), tensor([293, 277, 429, 427,   5, 489, 314,  40]), tensor([266, 320, 441, 370, 434, 359, 331, 251]), tensor([241, 458, 420, 302, 208, 476, 306,  90]), tensor([425,  32, 439, 459,  27, 425, 300,  27]), tensor([186, 408, 260, 352, 182, 115, 178,  18]), tensor([368, 299, 338,  16, 339, 401, 455, 282]), tensor([414, 299, 188, 327,  58, 190, 208, 463]), tensor([133,  25, 209, 167, 227, 430, 187, 122]), tensor([200, 452, 308, 338, 436, 376, 263, 413]), tensor([371, 249, 132, 324, 221, 307,  80, 178]), tensor([268, 423, 263, 359, 323, 289, 377, 400]), tensor([415, 301, 258, 355, 230, 198,  76, 282]), tensor([213, 320,  28, 352, 374, 117, 363, 397]), tensor([379, 134, 120,  15, 362, 340, 129, 203]), tensor([376, 396, 361,  89,  29, 276, 100, 311]), tensor([257, 286, 429, 328, 224, 295, 124,  98]), tensor([329,  41, 254, 175, 277, 180, 298, 463]), tensor([348,  73, 151,  57,  24, 380,  82, 316]), tensor([389, 474, 199,  70, 189, 392, 470, 409]), tensor([ 89,  19, 259,  18, 171, 320, 443, 169]), tensor([391,  71, 208,  51, 257, 256, 239,   9]), tensor([ 67, 437, 428, 276,  49, 372,  44, 183]), tensor([202, 242, 348, 426,  28, 171, 256, 456]), tensor([194, 162, 288, 233, 304, 252, 158, 193]), tensor([119,  53,  60, 110, 344, 111, 360,  14]), tensor([389, 179, 150, 469, 240, 267, 156, 171]), tensor([187, 351,  76, 120, 197, 485,  63,  45]), tensor([337, 120, 456,  55, 423,  93, 454, 295]), tensor([307, 234, 271, 235, 116, 143,  97, 163]), tensor([155, 447, 161, 369, 433,  68,  44, 223]), tensor([ 91, 333,  66, 220,  79,  48, 415, 392]), tensor([491, 116, 401, 231, 334, 201, 202, 164]), tensor([253,  98,  56, 395,  34,  58,  22, 432]), tensor([ 67, 203, 477, 100, 219, 133, 149, 143]), tensor([111, 141, 108,  26, 169, 148, 182, 121]), tensor([278, 259, 265, 375, 255,  63, 158, 399]), tensor([330,  36,  54, 452, 221,  50, 101, 364]), tensor([ 92, 332, 244, 475, 291, 188, 225, 217]), tensor([243, 435,  55, 353,  64,  16, 322, 181]), tensor([111, 276,  50, 139, 411, 321, 457, 270]), tensor([324,  81,  39, 479, 341, 405, 342, 385]), tensor([ 23, 176, 103, 373, 348,  84, 340,  93]), tensor([301, 220, 436, 240, 199,  99, 486, 152]), tensor([ 72, 452, 266, 354, 490, 397, 198, 400]), tensor([ 41,  12, 214, 407, 297,  33, 170, 404]), tensor([224, 376, 104, 211, 388, 420,  39, 179]), tensor([  5, 410, 242, 184,  11, 484, 482, 253]), tensor([140, 172,  96, 383, 329, 181, 272, 294]), tensor([337, 353, 293, 207,  13, 180, 157, 220]), tensor([298, 205,  34, 330,  31, 418, 339, 233]), tensor([166,  47, 164, 281, 154, 186, 129, 169]), tensor([104, 112, 386, 444, 260, 439, 449, 428]), tensor([ 13, 232, 433, 223, 322, 238,  19, 116]), tensor([166, 280, 411,  10, 384, 426, 185, 454]), tensor([394, 284,  59, 332, 295, 101, 378, 454]), tensor([160, 225, 193, 413,   0, 108, 392,  42]), tensor([  7, 332, 203,  40, 325, 461,   1,  38]), tensor([141, 330, 177,  30,   9,  44, 434,  82]), tensor([342, 479, 364, 466, 285, 430, 208, 444])]\n",
            "[1778 1290 1384 ... 2460 1738  327]\n",
            "Accuracy: 0.0010162601626016261\n",
            "Precision: 0.0012012012012012011\n",
            "Recall: 0.0003003003003003003\n",
            "F1-score: 0.0004719004719004719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "from sklearn import metrics\n",
        "\n",
        "dists = cdist(np.concatenate(test_data_map), np.concatenate(train_data_map))\n",
        "pred_labels = np.argmin(dists, axis=1)\n",
        "\n",
        "print(train_label_map)\n",
        "\n",
        "print(test_label_map)\n",
        "\n",
        "print(pred_labels)\n",
        "\n",
        "accuracy = np.mean(pred_labels == np.concatenate(test_label_map))\n",
        "\n",
        "accuracy\n",
        "\n",
        "precision = metrics.precision_score(np.concatenate(test_label_map), pred_labels, average='macro')\n",
        "recall = metrics.recall_score(np.concatenate(test_label_map), pred_labels, average='macro')\n",
        "f1_score = metrics.f1_score(np.concatenate(test_label_map), pred_labels, average='macro')\n",
        "\n",
        "print('Accuracy:', accuracy)\n",
        "print('Precision:', precision)\n",
        "print('Recall:', recall)\n",
        "print('F1-score:', f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([380, 315, 389, 275, 341, 447, 416, 271])\n",
            "tensor([[0.0000, 0.1946, 0.0000,  ..., 0.4340, 0.0000, 0.0000],\n",
            "        [0.0695, 0.3690, 0.0000,  ..., 0.1610, 0.0000, 0.3215],\n",
            "        [0.7957, 0.0000, 1.1393,  ..., 0.8630, 0.0000, 0.5614],\n",
            "        ...,\n",
            "        [0.0000, 0.2939, 0.0000,  ..., 0.9435, 0.0000, 0.1087],\n",
            "        [0.0000, 0.1365, 0.0000,  ..., 0.0000, 0.0000, 0.0603],\n",
            "        [0.0505, 0.0000, 0.1425,  ..., 1.2953, 0.0000, 0.0000]])\n",
            "tensor([  9, 419,  93, 504, 317, 445, 419,  89])\n",
            "tensor([217, 203, 185, 374, 220, 340, 206, 248])\n",
            "tensor([[0.0000, 0.0993, 0.0000,  ..., 0.2739, 0.0000, 0.0000],\n",
            "        [0.0000, 0.7192, 0.0000,  ..., 0.2104, 0.0000, 0.0000],\n",
            "        [0.1241, 0.1207, 0.0000,  ..., 0.5288, 0.0000, 0.2647],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 1.5024, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2282],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 2.2895, 0.0000, 0.0000]])\n",
            "tensor([ 88,   4, 200, 408, 405, 222, 141, 222])\n",
            "tensor([ 84, 180, 309,  76,  99, 174, 172, 184])\n",
            "tensor([[0.3685, 0.0000, 0.0000,  ..., 0.7823, 0.0621, 0.0000],\n",
            "        [0.1542, 0.2018, 0.0000,  ..., 0.1801, 0.0000, 0.0186],\n",
            "        [0.0000, 0.1541, 0.0000,  ..., 0.1717, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 1.8514, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.2112,  ..., 1.2827, 0.0000, 0.4188],\n",
            "        [0.0000, 0.2113, 0.0000,  ..., 0.5970, 0.0000, 0.2177]])\n",
            "tensor([178, 457, 222, 369, 178, 307, 156,  49])\n",
            "tensor([481, 366, 308,  73, 489, 227,  49, 453])\n",
            "tensor([[0.3498, 0.0000, 0.0623,  ..., 0.5471, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.7861, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0179, 0.0000,  ..., 0.6804, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.4970, 0.0000, 0.3309],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.2982, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0268, 0.0000,  ..., 0.2245, 0.0000, 0.0000]])\n",
            "tensor([148, 504, 156, 251, 410, 419,  23, 119])\n",
            "tensor([ 48, 399, 416, 164, 448, 488, 421, 280])\n",
            "tensor([[0.5222, 0.0000, 0.0000,  ..., 0.9496, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.2037, 0.0000, 0.0000],\n",
            "        [0.0000, 0.1616, 0.0000,  ..., 0.4515, 0.0000, 0.0508],\n",
            "        ...,\n",
            "        [0.0000, 0.6293, 0.0000,  ..., 0.5757, 0.0000, 0.1269],\n",
            "        [0.2286, 0.1699, 0.0000,  ..., 0.3090, 0.0000, 0.0000],\n",
            "        [0.1134, 0.1388, 0.0000,  ..., 0.7937, 0.0000, 0.0000]])\n",
            "tensor([ 90, 419,  23, 222, 488, 332, 156, 178])\n",
            "tensor([179, 357, 227, 399, 186, 358, 409, 169])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb Cell 17\u001b[0m in \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb Cell 17\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(out)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_dataloader:\n",
        "        print(labels)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        print(outputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        print(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.models import vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = vgg16(pretrained = False, )\n",
        "input_lastLayer = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(input_lastLayer, len(dataset_train.classes))\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum=0.9,weight_decay=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb Cell 20\u001b[0m in \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m negative \u001b[39m=\u001b[39m negative\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m anchor_embed \u001b[39m=\u001b[39m model(anchor)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m positive_embed \u001b[39m=\u001b[39m model(positive)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m negative_embed \u001b[39m=\u001b[39m model(negative)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m batch_all_triplet_loss(anchor_embed, positive_embed, negative_embed)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_array = []\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (anchor, positive, negative) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        anchor = anchor.to(device)\n",
        "        positive = positive.to(device)\n",
        "        negative = negative.to(device)\n",
        "        anchor_embed = model(anchor)\n",
        "        positive_embed = model(positive)\n",
        "        negative_embed = model(negative)\n",
        "        loss = batch_all_triplet_loss(anchor_embed, positive_embed, negative_embed)\n",
        "        loss_array.append(float(loss))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(model.state_dict(), 'model_vgg_' + str(epoch) + '.pth')\n",
        "    print ('Epoch [{}/{}], Loss: {}' \n",
        "                   .format(epoch+1, num_epochs, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
