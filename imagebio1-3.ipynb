{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cidSeyf9q3uu"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def clahe1(img, block_size=8, clip_limit=2.0):\n",
    "    # Convert the input image to grayscale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute the image size\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Compute the size of each small region\n",
    "    sx, sy = int(w/block_size), int(h/block_size)\n",
    "\n",
    "    # Initialize the output image\n",
    "    out = np.zeros_like(img)\n",
    "\n",
    "    # Loop over each small region\n",
    "    for i in range(block_size):\n",
    "        for j in range(block_size):\n",
    "            # Compute the coordinates of the current region\n",
    "            x, y = j*sx, i*sy\n",
    "\n",
    "            # Extract the current region\n",
    "            region = img[y:y+sy, x:x+sx]\n",
    "\n",
    "            # Compute the histogram for the current region\n",
    "            hist, bins = np.histogram(region.flatten(), 256, [0,256])\n",
    "            cdf = hist.cumsum()\n",
    "            cdf_normalized = cdf / cdf[-1]\n",
    "\n",
    "            # Compute the excess and clip the histogram\n",
    "            w = block_size*block_size\n",
    "            excess = 0\n",
    "            for k in range(256):\n",
    "                if cdf_normalized[k] > clip_limit/w:\n",
    "                    excess += cdf_normalized[k] - clip_limit/w\n",
    "                    cdf_normalized[k] = clip_limit/w\n",
    "            for k in range(256):\n",
    "                cdf_normalized[k] += excess/256\n",
    "\n",
    "            # Reallocate pixel values using the cdf\n",
    "            region_clahe = np.interp(region.flatten(), bins[:-1], cdf_normalized*255).reshape(region.shape)\n",
    "\n",
    "            # Insert the enhanced region into the output image\n",
    "            out[y:y+sy, x:x+sx] = region_clahe\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uWz8h_nXq9X5"
   },
   "outputs": [],
   "source": [
    "from numpy.lib.type_check import imag\n",
    "\n",
    "def clahe2(img):\n",
    "  image = cv2.resize(img, (500, 600))\n",
    "  \n",
    "  # The initial processing of the image\n",
    "  # image = cv2.medianBlur(image, 3)\n",
    "  image_bw = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  \n",
    "  # The declaration of CLAHE\n",
    "  # clipLimit -> Threshold for contrast limiting\n",
    "  clahe = cv2.createCLAHE(clipLimit = 5)\n",
    "  final_img = clahe.apply(image_bw) + 30\n",
    "  \n",
    "  # Ordinary thresholding the same image\n",
    "  _, ordinary_img = cv2.threshold(image_bw, 155, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "  med_final = cv2.medianBlur(final_img, 3)\n",
    "\n",
    "\n",
    "  return med_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WIWSLioCrN0A"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(img):\n",
    "# Load finger vein image\n",
    "  #img = cv2.imread('download.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "  # Convert image to vector form\n",
    "  img_1d = img.reshape(-1)\n",
    "\n",
    "  # Apply PCA\n",
    "  pca = PCA(n_components=0.5)\n",
    "  pca.fit(img_1d.reshape(-1, 1))\n",
    "  img_pca = pca.transform(img_1d.reshape(-1, 1)).reshape(img.shape[0], img.shape[1], -1)\n",
    "\n",
    "  return img_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = ['1st_session/raw_data', '2nd_session/raw_data']\n",
    "preprocessed_dir_prefix = 'preprocessed/'\n",
    "clahe_folder = 'clahe'\n",
    "pca_folder = 'pca'\n",
    "if not(os.path.exists(preprocessed_dir_prefix)): os.makedirs(preprocessed_dir_prefix)\n",
    "\n",
    "for folder in img_dir:\n",
    "    for folder_name in os.listdir(folder):\n",
    "        folder_path = os.path.join(folder, folder_name)\n",
    "        if '.DS_Store' in folder_name:\n",
    "            continue\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            if '.jpg' in img_name:\n",
    "                roi = cv2.imread(os.path.join(folder_path, img_name))\n",
    "                img1 = clahe2(roi)\n",
    "                f_img1 = pca(img1)\n",
    "                path_clahe = os.path.join(preprocessed_dir_prefix, clahe_folder, folder_path)\n",
    "                path_pca = os.path.join(preprocessed_dir_prefix, pca_folder, folder_path)\n",
    "                if not(os.path.exists(path_clahe)): os.makedirs(path_clahe)\n",
    "                if not(os.path.exists(path_pca)): os.makedirs(path_pca)\n",
    "                cv2.imwrite(os.path.join(path_clahe, img_name), img1)\n",
    "                cv2.imwrite(os.path.join(path_pca, img_name), f_img1)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGnet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class TripletImageDataset(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super(TripletImageDataset, self).__init__(root, transform, target_transform)\n",
    "        # self.triplets = self.make_triplets()\n",
    "\n",
    "    def make_triplets(self):\n",
    "        triplets = []\n",
    "        for index, (img, label) in enumerate(self.samples):\n",
    "            anchor_idx = index\n",
    "            \n",
    "            for i in range(2):\n",
    "\n",
    "                positive_indexes = [ind for ind, (_, label_positive) in enumerate(self.samples) if label == label_positive and ind != index]\n",
    "                positive_idx = random.choice(positive_indexes)\n",
    "\n",
    "                negative_indexes = [ind for ind, (_, label_positive) in enumerate(self.samples) if label != label_positive]\n",
    "                negative_idx = random.choice(negative_indexes)\n",
    "\n",
    "                triplets.append((anchor_idx, positive_idx, negative_idx))\n",
    "        return triplets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        # print(self.samples)\n",
    "        # print(len(self.triplets))\n",
    "        # print(self.triplets)\n",
    "        self.triplets = self.make_triplets()\n",
    "        anchor, positive, negative = self.triplets[index]\n",
    "        # print(anchor, positive, negative)\n",
    "        anchor_img, _ = super(TripletImageDataset, self).__getitem__(anchor)\n",
    "        positive_img, _ = super(TripletImageDataset, self).__getitem__(positive)\n",
    "        negative_img, _ = super(TripletImageDataset, self).__getitem__(negative)\n",
    "        return anchor_img, positive_img, negative_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Grayscale(), transforms.Resize((224, 224)), transforms.RandomRotation((0, 350)), transforms.ToTensor()])\n",
    "# transform = transforms.Compose([transforms.Resize((224, 224)), transforms.RandomRotation((0, 350)), transforms.ToTensor()])\n",
    "\n",
    "dataset_train = TripletImageDataset('preprocessed/clahe/1st_session/raw_data/', transform=transform)\n",
    "dataset_test = ImageFolder('preprocessed/clahe/2nd_session/raw_data/', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=16, shuffle=True)\n",
    "\n",
    "num_classes = len(dataset_train.classes)\n",
    "num_epochs = 40\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = VGGnet().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    \"\"\"\n",
    "    Compute Euclidean distance between two tensors.\n",
    "    \"\"\"\n",
    "    return torch.pow(x - y, 2).sum(dim=1)\n",
    "\n",
    "def compute_distance_matrix(anchor, positive, negative):\n",
    "    \"\"\"\n",
    "    Compute distance matrix between anchor, positive, and negative samples.\n",
    "    \"\"\"\n",
    "    distance_matrix = torch.zeros(anchor.size(0), 3)\n",
    "    distance_matrix[:, 0] = euclidean_distance(anchor, anchor)\n",
    "    distance_matrix[:, 1] = euclidean_distance(anchor, positive)\n",
    "    distance_matrix[:, 2] = euclidean_distance(anchor, negative)\n",
    "    return distance_matrix\n",
    "\n",
    "def batch_all_triplet_loss(anchor, positive, negative, margin=0.2):\n",
    "    \"\"\"\n",
    "    Compute triplet loss using the batch all strategy.\n",
    "    \"\"\"\n",
    "    distance_matrix = compute_distance_matrix(anchor, positive, negative)\n",
    "    loss = torch.max(torch.tensor(0.0), distance_matrix[:, 0] - distance_matrix[:, 1] + margin)\n",
    "    loss += torch.max(torch.tensor(0.0), distance_matrix[:, 0] - distance_matrix[:, 2] + margin)\n",
    "    print(loss)\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = F.pairwise_distance(anchor, positive, p=2)\n",
    "        distance_negative = F.pairwise_distance(anchor, negative, p=2)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return torch.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3665, 0.3680, 0.3662, 0.3678, 0.3685, 0.3684, 0.3700, 0.3669, 0.3692,\n",
      "        0.3677, 0.3642, 0.3687, 0.3677, 0.3682, 0.3652, 0.3651],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "loss_array = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (anchor, positive, negative) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = batch_all_triplet_loss(anchor_embed, positive_embed, negative_embed)\n",
    "        loss_array.append(float(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#     if epoch % 100 == 0 and epoch != 0:\n",
    "#         torch.save(model.state_dict(), 'model_vgg_checkpoint_' + str(epoch) + '.pth')\n",
    "    print ('Epoch [{}/{}], Loss: {}' \n",
    "                   .format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('finger_vein.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_vgg_checkpoint_20.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = ImageFolder('preprocessed/clahe/1st_session/raw_data/', transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_map = []\n",
    "train_label_map = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images).detach().numpy()\n",
    "        train_data_map.append(outputs)\n",
    "        train_label_map = np.concatenate((train_label_map, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_map = []\n",
    "test_label_map = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images).detach().numpy()\n",
    "        test_data_map.append(outputs)\n",
    "        test_label_map = np.concatenate((test_label_map, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 49.  24. 207. ... 147. 187. 361.]\n",
      "[478. 109. 284. ... 412. 486. 160.]\n",
      "[1935  151 2177 ...  384 1573  418]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/kthtqnpx4glb87k85hff26rc0000gn/T/ipykernel_8065/3138683164.py:12: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  accuracy = np.mean(pred_labels == np.concatenate(test_data_map))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "dists = cdist(np.concatenate(test_data_map), np.concatenate(train_data_map))\n",
    "pred_labels = np.argmin(dists, axis=1)\n",
    "\n",
    "print(train_label_map)\n",
    "\n",
    "print(test_label_map)\n",
    "\n",
    "print(pred_labels)\n",
    "\n",
    "accuracy = np.mean(pred_labels == np.concatenate(test_label_map))\n",
    "\n",
    "accuracy\n",
    "\n",
    "# precision = metrics.precision_score(np.concatenate(train_labels), pred_labels, average='macro')\n",
    "# recall = metrics.recall_score(np.concatenate(train_labels), pred_labels, average='macro')\n",
    "# f1_score = metrics.f1_score(np.concatenate(train_labels), pred_labels, average='macro')\n",
    "\n",
    "# print('Accuracy:', accuracy)\n",
    "# print('Precision:', precision)\n",
    "# print('Recall:', recall)\n",
    "# print('F1-score:', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([380, 315, 389, 275, 341, 447, 416, 271])\n",
      "tensor([[0.0000, 0.1946, 0.0000,  ..., 0.4340, 0.0000, 0.0000],\n",
      "        [0.0695, 0.3690, 0.0000,  ..., 0.1610, 0.0000, 0.3215],\n",
      "        [0.7957, 0.0000, 1.1393,  ..., 0.8630, 0.0000, 0.5614],\n",
      "        ...,\n",
      "        [0.0000, 0.2939, 0.0000,  ..., 0.9435, 0.0000, 0.1087],\n",
      "        [0.0000, 0.1365, 0.0000,  ..., 0.0000, 0.0000, 0.0603],\n",
      "        [0.0505, 0.0000, 0.1425,  ..., 1.2953, 0.0000, 0.0000]])\n",
      "tensor([  9, 419,  93, 504, 317, 445, 419,  89])\n",
      "tensor([217, 203, 185, 374, 220, 340, 206, 248])\n",
      "tensor([[0.0000, 0.0993, 0.0000,  ..., 0.2739, 0.0000, 0.0000],\n",
      "        [0.0000, 0.7192, 0.0000,  ..., 0.2104, 0.0000, 0.0000],\n",
      "        [0.1241, 0.1207, 0.0000,  ..., 0.5288, 0.0000, 0.2647],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.5024, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2282],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 2.2895, 0.0000, 0.0000]])\n",
      "tensor([ 88,   4, 200, 408, 405, 222, 141, 222])\n",
      "tensor([ 84, 180, 309,  76,  99, 174, 172, 184])\n",
      "tensor([[0.3685, 0.0000, 0.0000,  ..., 0.7823, 0.0621, 0.0000],\n",
      "        [0.1542, 0.2018, 0.0000,  ..., 0.1801, 0.0000, 0.0186],\n",
      "        [0.0000, 0.1541, 0.0000,  ..., 0.1717, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.8514, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.2112,  ..., 1.2827, 0.0000, 0.4188],\n",
      "        [0.0000, 0.2113, 0.0000,  ..., 0.5970, 0.0000, 0.2177]])\n",
      "tensor([178, 457, 222, 369, 178, 307, 156,  49])\n",
      "tensor([481, 366, 308,  73, 489, 227,  49, 453])\n",
      "tensor([[0.3498, 0.0000, 0.0623,  ..., 0.5471, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.7861, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0179, 0.0000,  ..., 0.6804, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.4970, 0.0000, 0.3309],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.2982, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0268, 0.0000,  ..., 0.2245, 0.0000, 0.0000]])\n",
      "tensor([148, 504, 156, 251, 410, 419,  23, 119])\n",
      "tensor([ 48, 399, 416, 164, 448, 488, 421, 280])\n",
      "tensor([[0.5222, 0.0000, 0.0000,  ..., 0.9496, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.2037, 0.0000, 0.0000],\n",
      "        [0.0000, 0.1616, 0.0000,  ..., 0.4515, 0.0000, 0.0508],\n",
      "        ...,\n",
      "        [0.0000, 0.6293, 0.0000,  ..., 0.5757, 0.0000, 0.1269],\n",
      "        [0.2286, 0.1699, 0.0000,  ..., 0.3090, 0.0000, 0.0000],\n",
      "        [0.1134, 0.1388, 0.0000,  ..., 0.7937, 0.0000, 0.0000]])\n",
      "tensor([ 90, 419,  23, 222, 488, 332, 156, 178])\n",
      "tensor([179, 357, 227, 399, 186, 358, 409, 169])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb Cell 17\u001b[0m in \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb Cell 17\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X21sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(out)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_dataloader:\n",
    "        print(labels)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        print(outputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg16(pretrained = False, )\n",
    "input_lastLayer = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(input_lastLayer, len(dataset_train.classes))\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum=0.9,weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb Cell 20\u001b[0m in \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m negative \u001b[39m=\u001b[39m negative\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m anchor_embed \u001b[39m=\u001b[39m model(anchor)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m positive_embed \u001b[39m=\u001b[39m model(positive)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m negative_embed \u001b[39m=\u001b[39m model(negative)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/Documents/Sem2/Biometrics/Project/Biometrics_Project/imagebio1.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m batch_all_triplet_loss(anchor_embed, positive_embed, negative_embed)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m     67\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_array = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (anchor, positive, negative) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        anchor_embed = model(anchor)\n",
    "        positive_embed = model(positive)\n",
    "        negative_embed = model(negative)\n",
    "        loss = batch_all_triplet_loss(anchor_embed, positive_embed, negative_embed)\n",
    "        loss_array.append(float(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), 'model_vgg_' + str(epoch) + '.pth')\n",
    "    print ('Epoch [{}/{}], Loss: {}' \n",
    "                   .format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
